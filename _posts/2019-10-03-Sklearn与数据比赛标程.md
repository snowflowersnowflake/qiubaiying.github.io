---
layout:     post
title:      Sklearn与数据比赛标程
subtitle:   #
date:       2019-10-03
author:     y00
header-img: img/asuna.jpg
catalog: true
tags:
    - python
    - Sklearn
    - 练习题
    - 机器学习
---

<iframe
  frameborder="no"
  border="0"
  marginwidth="0"
  marginheight="0"
  width="330"
  height="86"
  src="//music.163.com/outchain/player?type=2&id=27867500&auto=0&height=66"
></iframe>

# sklearn

sklearn提供了丰富的机器学习模型，例如回归、聚类、简单神经网络模型，等等，并且封装便于使用，处理数据上十分便捷

用conda或者pip可以安装

sklearn在数据竞赛中十分常用，对于基本的结构化数据题，有时不用几行代码就可以完成dataset-model-preResult的分析过程

因此本文中，主要利用数据竞赛标程来展示sklearn这个强大的机器学习库

以下标程均不用做特征工程和模型融合，因此作为sklearn的入门教程

btw，如果需要练习的数据集，可以在sklearn.dataset中获取

[官网提供dataset与example](https://scikit-learn.org/stable/auto_examples/#examples-based-on-real-world-datasets)

# 标程（全部来源于[sofasofa.io](http://sofasofa.io/tutorials/sofa_benchmark/)的练习题）

**本文在代码注释中加上了丰富的解释**

同时，涉及到的模型并不全面，但作为参考，可以同理应用库里的其他模型，后续或许会继续完善补充知识点

## 共享单车被借走数量预测(回归问题)

> 本练习赛的数据取自于两个城市某街道上的几处公共自行车停车桩。我们希望根据时间、天气等信息，预测出该街区在一小时内的被借取的公共自行车的数量。 


d  |	行编号，没有实际意义
--  |   --
y	|一小时内自行车被借取的数量。在test.csv中，这是需要被预测的数值。
city	|表示该行记录所发生的城市，一共两个城市
hour	|当时的时间，精确到小时，24小时计时法
is_workday	|1表示工作日，0表示节假日或者周末
temp_1	|当时的气温，单位为摄氏度
temp_2	|当时的体感温度，单位为摄氏度
weather	|当时的天气状况，1为晴朗，2为多云、阴天，3为轻度降水天气，4为强降水天气
wind	|当时的风速，数值越大表示风速越大


### 线性回归模型

如果你有 1，1    2，2     3，3    4，4      5，5 
    
这样的五个坐标，用一根线去拟合，那就会得到y=x
    
线性回归就是这么一回事，用一个线性函数拟合你的train，然后如果要预测x=k的地方对应的值，就带入这个函数。


```python

# -*- coding: utf-8 -*-

# 引入模块 LinearRegression线性回归和pandas清洗数据
from sklearn.linear_model import LinearRegression
import pandas as pd

# 读取数据 pd.read_csv可以从题目给出的.csv数据集中获取到数据 submit为提交样板
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")
submit = pd.read_csv("sample_submit.csv")

# 删除id 去掉pd_dataframe中的id列，便于建模
train.drop('id', axis=1, inplace=True)
test.drop('id', axis=1, inplace=True)

# 取出训练集的y pop出最后一列的y标签，便于在后面拟合x
y_train = train.pop('y')

# 建立线性回归模型 reg.fit需要训练集的x以及对应的y
reg = LinearRegression()
reg.fit(train, y_train)
y_pred = reg.predict(test) #然后用训练好的reg，导入test进行predict操作

# 若预测值是负数，则取0  进行lambda匹配，消灭不合理的预测结果
y_pred = map(lambda x: x if x >= 0 else 0, y_pred)

# 输出预测结果至my_LR_prediction.csv
submit['y'] = y_pred
submit.to_csv('my_LR_prediction.csv', index=False)

```
### 决策树模型

用经典的栗子来说，决策树就是：

女生要找对象，进行决策

* 不帅->0
* 帅->程序员->0
* 帅->不是程序员->没房没车->0
* 帅->不是程序员->有房有车->1

当然在许多模型中，决策可能是连续的区间分割，分类出的标签也可能不止0和1

```python
# -*- coding: utf-8 -*-

# 引入模块 
from sklearn.tree import DecisionTreeRegressor
import pandas as pd

# 读取数据
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")
submit = pd.read_csv("sample_submit.csv")

# 删除id
train.drop('id', axis=1, inplace=True)
test.drop('id', axis=1, inplace=True)

# 取出训练集的y
y_train = train.pop('y')

# 建立最大深度为5的决策树回归模型 可以认为是决策的次数，每次决策其数据结构上来看图（树）就多出一层
reg = DecisionTreeRegressor(max_depth=5)
reg.fit(train, y_train)
y_pred = reg.predict(test)

#决策树模型会依照数据的标签为每个条件进行决策分类

# 输出预测结果至my_DT_prediction.csv
submit['y'] = y_pred
submit.to_csv('my_DT_prediction.csv', index=False)

```

## 交通事故理赔审核 （二元分类问题）

> 在交通摩擦（事故）发生后，理赔员会前往现场勘察、采集信息，这些信息往往影响着车主是否能够得到保险公司的理赔。

> 我们需要根据Q1-Q36这36条信息预测该事故方没有被理赔的概率。

CaseId	|案例编号，没有实际意义
--|--
Q1	|赔员现场勘察采集的信息，Q1代表第一个问题的信息。信息被编码成数字，数字的大小不代表真实的关系。
Qk	|上，Qk代表第k个问题的信息。一共36个问题。
Evaluation|表示最终审核结果。0表示授予理赔，1表示未通过理赔审核。在test.csv中，这是需要被预测的标签。

### LASSO逻辑回归模型

逻辑回归就是线性回归的二元分类版本

线性回归输出一个y作为预测值，在逻辑回归中通过建立映射，输出y对应的一个几率

然后通过这个几率进行 true or false 的逻辑判断

lasso是一中逻辑回归模型的方法，具体的公式推导可以自行查阅

```python

# -*- coding: utf-8 -*-

import pandas as pd
from sklearn.linear_model import LogisticRegression

# 读取数据
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")
submit = pd.read_csv("sample_submit.csv")

# 删除id
train.drop('CaseId', axis=1, inplace=True)
test.drop('CaseId', axis=1, inplace=True)

# 取出训练集的y
y_train = train.pop('Evaluation')

# 建立LASSO逻辑回归模型 penalty代表正则化方法
# L1规范假设的是模型的参数满足拉普拉斯分布，L2假设的模型参数满足高斯分布
# 一般来说样本数据服从x分布，那么采用的统计方法也要基于这个x分布，事实上在统计学的分析方式中相当讲究分布的原理和假设检验
# 参数c表示正则化系数，越大意味着泛化程度越好，避免过拟合，但同时也代表模型的准确率可能会下降
# random_state表示随机数种子 随机数种子可以认为是一个生成器 0表示无种子
# 打比方，在实际试验中，如果使用同样的随机数种子进行连续两次随机，那么这两次生成的序列是“完全一样的”
clf = LogisticRegression(penalty='l1', C=1.0, random_state=0)
clf.fit(train, y_train)
y_pred = clf.predict_proba(test)[:, 1]

# 输出预测结果至my_LASSO_prediction.csv
submit['Evaluation'] = y_pred
submit.to_csv('my_LASSO_prediction.csv', index=False)

```

### 随机森林模型

其实就是衍生版本的决策树算法

随机森林会根据样本生成n个决策树，每个决策树覆盖的范围各不相同，但最终都会覆盖全部的特征。

因为有多个决策树的输出结果，因此采取诸如投票等方式决定最后的标签。

如果说是决策树为 决策-结果 的映射

那么随机森林其实就是 决策森林-多结果-最终结果 的映射

```python

# -*- coding: utf-8 -*-
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

# 读取数据
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")
submit = pd.read_csv("sample_submit.csv")

# 删除id
train.drop('CaseId', axis=1, inplace=True)
test.drop('CaseId', axis=1, inplace=True)

# 取出训练集的y
y_train = train.pop('Evaluation')

# 建立随机森林模型 n_estimators代表森林数量，默认参数是10
clf = RandomForestClassifier(n_estimators=100, random_state=0)
clf.fit(train, y_train)
y_pred = clf.predict_proba(test)[:, 1] #预测结果是n个二维组，对预测结果取[:, 1]实际上就是返回每组的第二个值
#也就是说，得到的每组01分类中，1的那一类被切片出来，用来作为结果输出到提交答案中

# 输出预测结果至my_RF_prediction.csv
submit['Evaluation'] = y_pred
submit.to_csv('my_RF_prediction.csv', index=False)

```

## 机器读中文：根据名字判断性别（自然语言处理、二元分类）

> 作为一个说着中国话的中国人，我们有着常年累月的积累和对文字的理解，通过名字判断（猜测）性别，并非难事。可是，对于一个冰冷的机器，它能够根据名字判断性别吗？这个练习赛就是根据中文名字（姓已经省略），判断性别 


变量名|	解释
--|--
id	|编号
name	|名字（姓已经被隐去）
gender | 表示该姓名对应的性别。0表示女生，1表示男生。在test.txt中，这是需要被预测的标签。请注意：名字与性别均来自于网络爬虫，数据可能有杂质或者重复。

### 朴素贝叶斯分类模型

就是基于样本特征的概率密度做朴素分类，nlp模型中朴素贝叶斯统计思想常常出现

#### 基于TF的GBDT模型

后面代码中GradientBoostingClassifier就是基于GBDT（梯度提升决策树）的分类器

GBDT是一种集成学习算法，在决策树的基础上改良。通过对loss函数梯度方向提升来优化决策树的结构

GBDT利用梯度提升的思想，在loss函数梯度（上升下降最快）的方向上优化决策树的结构

相较于普通决策树，GBDT更不容易造成过拟合现象，对特征工程的依赖性也更小

算法每次迭代生成一颗新的决策树 

在每次迭代开始之前，计算损失函数在每个训练样本点的一阶导数和二阶导数 

通过贪心策略（从0深度开始每次线性扫描出最佳收益的分裂点）生成新的决策树，通过计算每个叶节点对应的预测值

把新生成的决策树添加到模型中

```python
# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
#collections提供了集合类，可以进行对应的数据操作
from collections import Counter
from sklearn.ensemble import GradientBoostingClassifier

# 读取数据
train = pd.read_table('train.txt', ',')
test = pd.read_table('test.txt', ',')
submit = pd.read_csv('sample_submit.csv')

# 所有男生的名字
train_male = train[train['gender'] == 1]
m_cnt = len(train_male)
names_male = "".join(train_male['name'])

# 所有女生的名字
train_female = train[train['gender'] == 0]
f_cnt = len(train_female)
names_female = "".join(train_female['name'])

# 统计每个字在男生、女生名字中出现的总次数
lists_male = map(lambda x: x.encode('utf-8'), names_male.decode('utf-8')) #用map对male中的每个汉字进行解码
counts_male = Counter(lists_male) #counter是collections中的一种高性能数据类型，可以提供便携的统计方式，这里对其转换
lists_female = map(lambda x: x.encode('utf-8'), names_female.decode('utf-8'))
counts_female = Counter(lists_female)

# 得到训练集中每个人的每个字的词频（Term Frequency，通常简称TF）
train_encoded = []
for i in range(len(train)):
    name = train.at[i, 'name']
    chs = map(lambda x: x.encode('utf-8'), name.decode('utf-8'))
    row = [0., 0., 0., 0, train.at[i, 'gender']]
    for j in range(len(chs)): #针对训练集中的名字，对每个字在之前统计中的词频做商，得到基于朴素贝叶斯的统计概率
        row[2* j] = counts_female[chs[j]] * 1. / f_cnt
        row[2* j + 1] = counts_male[chs[j]] * 1. / m_cnt
    train_encoded.append(row)

# 得到测试集中每个人的每个字的词频（Term Frequency，通常简称TF）
test_encoded = []
for i in range(len(test)):
    name = test.at[i, 'name']
    chs = map(lambda x: x.encode('utf-8'), name.decode('utf-8'))
    row = [0., 0., 0., 0.,]
    for j in range(len(chs)):
        try:
            row[2 * j] = counts_female[chs[j]] * 1. / f_cnt
        except:
            pass
        try:
            row[2 * j + 1] = counts_male[chs[j]] * 1. / m_cnt
        except:
            pass
    test_encoded.append(row)

# 转换为pandas.DataFrame的形式
# 1_f是指这个人的第一个字在训练集中所有女生的字中出现的频率
# 2_f是指这个人的第二个字在训练集中所有女生的字中出现的频率
# 1_m是指这个人的第一个字在训练集中所有男生的字中出现的频率
# 2_m是指这个人的第二个字在训练集中所有男生的字中出现的频率
train_encoded = pd.DataFrame(train_encoded, columns=['1_f', '1_m', '2_f', '2_m', 'gender'])
test_encoded = pd.DataFrame(test_encoded, columns=['1_f', '1_m', '2_f', '2_m'])

# 训练GBDT模型

clf = GradientBoostingClassifier()
clf.fit(train_encoded.drop('gender', axis=1), train_encoded['gender'])
preds = clf.predict(test_encoded)

# 输出预测结果至my_TF_GBDT_prediction.csv
submit['gender'] = np.array(preds)
submit.to_csv('my_TF_GBDT_prediction.csv', index=False)

```
