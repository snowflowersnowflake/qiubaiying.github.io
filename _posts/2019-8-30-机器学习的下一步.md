---
layout:     post
title:      机器学习的下一步
subtitle:   #
date:       2019-8-30
author:     y00
header-img: img/=-=.png
catalog: true
tags:
    - 机器学习
    - 心得笔记
---

# 导语

说起机器学习，你应该最容易将神经网络（network）和ai模型作为你想关注的重点内容。

如果用一句话概括**神经网络**在人工智能中的形象，那就是一个**函数生成器**（基于你给出的训练集x和y，生成x->y的映射过程）

它不断改变自己，然后变成一个效果最好的function，架起input->output之间的桥梁。

以图片识别举例，你的network模型事实上只是担当了一个输出“给定样本”和“标签label”之间的相似度（置信度）。用数学的角度，完全可以理解为:

**Network->Function(x)={置信度（label1），置信度（label2）….**

没错，第一个箭头之间的奥秘就在于你提供了大量的训练集作为支撑

你的训练集越好，function预测效果就越好。

可是我们不管是对神经网络算法掌握的多么透彻，也并不能让程序成为真正意义上的“智能”。

正如假期中你可能很快就上手了easydl平台，但你发现这不是开始也不是结束

你发现“一个有作用的程序”和一个“可用程序”相差甚远。

你会发现用easydl平台的简便功能你可以快速训练模型识别红绿灯的变化和马路上的标识，可是如果让你去做一个自动驾驶系统，你能建立一套思路来确定你的各类“训练目标”吗？

你能确保各种异常情况下，例如一个人在路边招手车辆不会出车祸吗？如何确保一个在线系统的安全性呢？

现实的残酷往往会给人提出很多思考。

是的，神经网络不是人工智能全貌，尽管它发展很快

可他作用只是对两个东西的相似程度做出一个分类判断，可以是文字、声音、图片等等。

我们许许多多可见的智能系统也仅仅止步于此，但谁也不会相信一个分类机器就是智能的。

如果失去“人的智慧”，~~人工智能~~它依然很智障。

未来，**还需要更多的，热点词汇之外的理论知识，和技术发展，去支撑一个宏图愿景。**

一个真正意义上的智能系统，不光要有大量的软硬件设计，还要有从人性化角度出发思考的要点和需要完善的地方。

**也是时下真正需要许许多多人去探索的地方**

## 机器能不能知道“我不知道”

神经网络往往可以对你进行的分类做出预测，例如你创建两个标签猫和狗，那么系统就只能对相比较“猫”和“狗”谁更像做判断，而不能判断之外的事物。

那么如果有一张海豚照片机器能不能说出“我不知道”呢？

在实际操作中我们常常会误认为将其他的集合一起做标签训练就好。例如设计这样的标签:**“猫”，“狗”，“猪”….“非动物集”**

这样的设计其实是相当不科学的，因为同样的集内的数据必须有相似的特征，“非集”的设计一方面带来了不稳定的结果，一方面造成“无法取到足够的非集（异常集）样本”的困难。

通俗的说就是：**那么问题来了，不是动物的集合包括哪些东西？**

如何针对这方面进行系统设计，**让系统能判断你给一个测试样本，是否不在你给定的标签里面，专业的说就是“[异常检测问题](https://www.cnblogs.com/fengfenggirl/p/iForest.html)”。**


有兴趣的同学可以深入了解学习，如何用异常检测算法进行样本分析

例如常用的，我们可以对置信度设置阈值，将样本输入network分类器得到置信度进行异常检测

以及通过[最大似然函数法](http://www.elecfans.com/baike/wangluo/zhuanhuanqi/20171201590509_a.html)直接将测试数据与训练样本进行相似度比较

这些在许多老师的讲课视频中，例如李宏毅、吴恩达的讲课中会提到。


值得关注的是由我们国人学者周志华提出的[“孤立森林法”](https://www.cnblogs.com/yinghuali/p/9260291.html)，较为新颖前沿，引起许多人关注：


## 机器能不能说出“为什么知道”

有一个神马汉斯的故事：

古代有一个被人称作天才的马。人们发现它会算数，每次大家聚在一起围观训练师和汉斯表演算数。

训练师问汉斯根号四是多少，汉斯跺脚两下，人们欢呼汉斯真是个小天才。

事实上后来被发现，汉斯只是学会了观察周围人的动静，每次做算数人们欢呼时就停止跺脚。

神经网络就属于一个黑箱模型，我们很难去证明这次我们取得了良好的结果是不是“汉斯的秘密”。

不管你取用网上怎样的算法做训练也好，神经网络能反馈给你的仅仅只是概率和数值，而不能讲述它能反馈正确结果的原因

即使最专业的学者也很难讲述在程序跑了这么些遍其中的内容到底经历了什么。

所以**如何让机器指出识别的方法，也是一大难题。**

也因此，我们需要对待数据来源十分谨慎。

我们假期实践中可能会犯下这样的错误：结果识别的置信度非常之高，然而机器只是依靠我们样本图片上“有没有网址”这样的特征来进行判断。


## 机器错觉

机器错觉实际上和安全性紧密关联

例如：一个识别器对一个图片和“熊猫”相似的置信度是60%

我们将其和一个噪点图合成，然后再次输入进系统，检测结果或许会出现：

和“狗”相似的置信度是90%

这样明显的荒谬结果不仅时常存在，而且永远潜在。

事实上，通常人工智能对抗的攻击方式也是如此，针对已有网络，对一个样本数据进行微小的改变，就能扭转输出结果。

比如经常被提到的[**一个像素的攻击**](http://www.sohu.com/a/289990451_822985)

**机器错觉的安全隐患是相当大的**。

可以试想，如果一个已经试行的自动驾驶汽车正载人行驶在城市一环线

有一个小孩子举着气球走过

智能系统将其视为一个红灯

出了一场车祸

这是多么可怕的一件事情。

## 终生学习问题

你如果看过一些关于ai方面的公众号，一定对阿尔法狗不陌生。

该公司的alpGo能在围棋上战胜李世石。

还研制过星际争霸AI alpStar 机器人，其强度能打职业比赛。

尽管这些能力很强，成为热点，让人瞠目结舌。

但我们也可以从中发现现有技术的局限性：

不同模型能完成的事情无法互通。

**那么能否让一个机器人不断学习，达成多任务处理能力，同时办到各种事情呢？**

有不少专业[书籍](https://bbs.pinggu.org/thread-6626320-1-1.html)对于此前沿领域展开了讨论。

## 学习如何学习

如果技术更新，能否让智能系统用算法设计算法。

即做一个“会自我编写学习算法的智能机器”。

如果能实现，无疑是一场科技革命。

虽然很遗憾，现有话题下这样的事情只在许多科幻作品中出现，但这或许是人工智能领域发展的一个“终极目标”

## 一定需要很多训练资料吗

用人脸识别举例说明[这个深度学习领域内研究热点](https://www.zhihu.com/question/50996014)

few-shot learning: 用一张人脸图片训练模型识别一个人

zero-shot learning:仅仅通过文字描述识别一个人脸图片

### 补充链接

[吴恩达教学视频](https://www.bilibili.com/video/av9912938?from=search&seid=3759716963061965663)
[李宏毅深度学习基础 网易云课堂免费网课](https://study.163.com/series/1202814602.htm)
[2019年李宏毅深度学习公开课（需要基础）]（https://www.bilibili.com/video/av48285039?from=search&seid=6020517737951115274）
[知乎回答中对机器学习基本概念和算法的归纳](https://www.zhihu.com/question/336275116/answer/759177861)
[知乎专栏之cnn卷积网络]（https://zhuanlan.zhihu.com/ligaoyi）
 
