---
layout:     post
title:      pandas进阶操作
subtitle:   #
date:       2019-9-22
author:     y00
header-img: img/ayano.jpg
catalog: true
tags:
    - python
    - pandas
    - 机器学习
---



<iframe
  frameborder="no"
  border="0"
  marginwidth="0"
  marginheight="0"
  width="330"
  height="86"
  src="//music.163.com/outchain/player?type=2&id=35505031&auto=0&height=66"
></iframe>





# 引言
Pandas 是基于 NumPy 的一种数据处理工具，该工具为了解决数据分析任务而创建。

Pandas 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的函数和方法。

对于机器学习算法的练习题来说，pandas更是必不可少，往往决定正确率的是特征工程而不是使用的算法模型。

pandas常常和numpy一起使用，进行数据的采样与处理。

# Pandas 的数据结构
Pandas 主要有 Series（一维数组），DataFrame（二维数组），Panel（三维数组），Panel4D（四维数组），PanelND（更多维数组）等数据结构。

其中 Series 和 DataFrame 应用的最为广泛。

* Series 是一维带标签的数组，它可以包含任何数据类型。包括整数，字符串，浮点数，Python 对象等。Series 可以通过标签来定位。
* DataFrame 是二维的带标签的数据结构。我们可以通过标签来定位数据。这是 NumPy 所没有的。

#导入所需要使用到的库

```python
import pandas as pd
import numpy as np
print(pd.__version__)#打印版本
```

# pandas进阶操作
基础的数据操作观看实例比照就能理解，进阶部分笔者会补充部分解释信息

# 时间序列索引

## 建立一个以 2018 年每一天为索引，值为随机数的 Series

```python
dti = pd.date_range(start='2018-01-01', end='2018-12-31', freq='D')
s = pd.Series(np.random.rand(len(dti)), index=dti)
print (s)
```

设置时间的起始和结束freq控制随机数的间隔频率，采用"D"的时候打印每天为单位的信息，然后将此作为index

np.random.rand返回【0,1】范围内的一个随机数

返回随机数的个数是len(dti)，以使每个Series中的索引都能匹配到一个随机数。

```txt
2018-01-01    0.289422
2018-01-02    0.322898
2018-01-03    0.663779
2018-01-04    0.513682
2018-01-05    0.996929
2018-01-06    0.303355
2018-01-07    0.466621
2018-01-08    0.079008
2018-01-09    0.775039
2018-01-10    0.198569
2018-01-11    0.401680
2018-01-12    0.126774
2018-01-13    0.875948
2018-01-14    0.086280
2018-01-15    0.048911
2018-01-16    0.687690
2018-01-17    0.765243
2018-01-18    0.623987
2018-01-19    0.407055
2018-01-20    0.802661
2018-01-21    0.644146
2018-01-22    0.511044
2018-01-23    0.426378
2018-01-24    0.769499
2018-01-25    0.011949
2018-01-26    0.667829
2018-01-27    0.295917
2018-01-28    0.114008
2018-01-29    0.201223
2018-01-30    0.791230
                ...   
2018-12-02    0.981167
2018-12-03    0.418215
2018-12-04    0.553704
2018-12-05    0.503421
2018-12-06    0.038832
2018-12-07    0.837607
2018-12-08    0.250743
2018-12-09    0.011625
2018-12-10    0.107244
2018-12-11    0.689305
2018-12-12    0.956466
2018-12-13    0.185858
2018-12-14    0.482651
2018-12-15    0.913743
2018-12-16    0.517277
2018-12-17    0.851381
2018-12-18    0.756169
2018-12-19    0.605364
2018-12-20    0.652177
2018-12-21    0.877070
2018-12-22    0.077671
2018-12-23    0.117162
2018-12-24    0.186197
2018-12-25    0.019362
2018-12-26    0.700295
2018-12-27    0.552401
2018-12-28    0.322484
2018-12-29    0.799141
2018-12-30    0.028285
2018-12-31    0.153351
Freq: D, Length: 365, dtype: float64
```
返回结果中会省略中间过程，展示头尾，底部length：表明Series中有365行，内容为浮点数

## 统计s中每一个周三对应值的和

```python
# 周一从 0 开始
print(s[s.index.weekday == 2].sum())
print()
```

```txt
27.832158328241547
```
用sum方法对s中索引为周三的内容求和打印即可，需要注意weekday从0计数

## 统计s中每个月值的平均值

```python
print(s.resample('M').mean())
print()
```

resample方法可以进行重采样

‘M’表示按照月份的方式进行时间重采样，那么原Series中的数据只会保留每月一行

mean方法用来求均值

```txt
2018-01-31    0.449804
2018-02-28    0.464638
2018-03-31    0.484023
2018-04-30    0.486435
2018-05-31    0.558915
2018-06-30    0.540861
2018-07-31    0.545044
2018-08-31    0.512989
2018-09-30    0.483962
2018-10-31    0.456614
2018-11-30    0.523259
2018-12-31    0.488175
Freq: M, dtype: float64
```

结果中对月末进行了重采样

也可以在对应的文档中可以找到该方法的说明，按照自定义的方式来重采样

## 时间转换 s->min

```python
s=pd.date_range('today',periods=100,freq='S')
ts=pd.Series(np.random.randint(0,500,len(s)),index=s)
print(ts)
print()
print(ts.resample('Min').sum())
print()
```

today表示从此刻电脑时间开始采样，以秒为单位，产生的序列赋值给s

然后s作为索引，randint生成随机的整数匹配每一个索引，产生Series指派给ts

然后按照Min的频率重采样，求和

```txt
2019-09-22 17:55:33.806691    434
2019-09-22 17:55:34.806691    254
2019-09-22 17:55:35.806691    105
2019-09-22 17:55:36.806691    114
2019-09-22 17:55:37.806691     39
2019-09-22 17:55:38.806691    127
2019-09-22 17:55:39.806691    342
2019-09-22 17:55:40.806691    117
2019-09-22 17:55:41.806691    120
2019-09-22 17:55:42.806691    179
2019-09-22 17:55:43.806691    488
2019-09-22 17:55:44.806691    140
2019-09-22 17:55:45.806691    375
2019-09-22 17:55:46.806691    277
2019-09-22 17:55:47.806691      7
2019-09-22 17:55:48.806691    212
2019-09-22 17:55:49.806691    433
2019-09-22 17:55:50.806691     67
2019-09-22 17:55:51.806691    392
2019-09-22 17:55:52.806691    361
2019-09-22 17:55:53.806691    302
2019-09-22 17:55:54.806691    189
2019-09-22 17:55:55.806691    301
2019-09-22 17:55:56.806691    494
2019-09-22 17:55:57.806691     35
2019-09-22 17:55:58.806691    135
2019-09-22 17:55:59.806691     69
2019-09-22 17:56:00.806691    332
2019-09-22 17:56:01.806691    111
2019-09-22 17:56:02.806691    325
                             ... 
2019-09-22 17:56:43.806691    337
2019-09-22 17:56:44.806691    438
2019-09-22 17:56:45.806691    452
2019-09-22 17:56:46.806691    289
2019-09-22 17:56:47.806691    306
2019-09-22 17:56:48.806691    187
2019-09-22 17:56:49.806691      3
2019-09-22 17:56:50.806691    156
2019-09-22 17:56:51.806691    322
2019-09-22 17:56:52.806691    210
2019-09-22 17:56:53.806691    117
2019-09-22 17:56:54.806691    247
2019-09-22 17:56:55.806691    322
2019-09-22 17:56:56.806691     89
2019-09-22 17:56:57.806691    134
2019-09-22 17:56:58.806691    116
2019-09-22 17:56:59.806691    416
2019-09-22 17:57:00.806691    347
2019-09-22 17:57:01.806691     62
2019-09-22 17:57:02.806691    375
2019-09-22 17:57:03.806691    458
2019-09-22 17:57:04.806691    460
2019-09-22 17:57:05.806691    491
2019-09-22 17:57:06.806691     62
2019-09-22 17:57:07.806691    307
2019-09-22 17:57:08.806691    313
2019-09-22 17:57:09.806691    426
2019-09-22 17:57:10.806691    403
2019-09-22 17:57:11.806691     21
2019-09-22 17:57:12.806691     15
Freq: S, Length: 100, dtype: int32

2019-09-22 17:55:00     6108
2019-09-22 17:56:00    15275
2019-09-22 17:57:00     3740
Freq: T, dtype: int32
```

## UTC 世界时间标准

UTC是协调世界时，具体内涵可以搜索了解。

这里仅仅作为一个时区示例，也可以用于本地化GET,CET等等时区


```python
s = pd.date_range('today', periods=1, freq='D')  # 获取当前时间
ts = pd.Series(np.random.randn(len(s)), s)  # 随机数值
print(ts)
ts_utc = ts.tz_localize('UTC')  # 转换为 UTC 时间
print(ts_utc)
```

获取当前的一个时间，匹配一个随机数

只需要用到ts.tz_localize时间本地化方法，可以按照指定时区进行本地化

可以注意到本地化前后时间内容是不变的，只是指派现在的时区是UTC

```txt
2019-09-22 18:57:49.747316   -0.377645
Freq: D, dtype: float64
2019-09-22 18:57:49.747316+00:00   -0.377645
Freq: D, dtype: float64
```

[这个文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.tz_localize.html)中包括了本地化和时区转换的使用细则

##  转换为上海所在时区

```python
ts_utc.tz_convert('Asia/Shanghai')
```

```txt
2019-09-23 02:57:49.747316+08:00   -0.377645
Freq: D, dtype: float64
```

之前我们将时区本地化到了UTC时区

利用convert就可以按照指定规则进行时区转换

通过字段可以了解到我们从UTC（假设现在本地时区是UTC）转到上海

得到转换后的时间

## 不同时间表示方式的转换

pandas的时间对象包括：
* timestamp时间点 表示一个具体的时间点，精确到秒
* period时期  表示某个时期，通常单位是某年、某月、某日、某小时
* timedetla时间段  表示一段时间间隔

关于这三种对象包含的方法和详细使用可以自行了解

三种对象之间可以相互转换

利用对象本身的方法，例如

```python
rng = pd.date_range('1/1/2018', periods=5, freq='M')
ts = pd.Series(np.random.randn(len(rng)), index=rng)
print(ts)
print()
ps = ts.to_period()
print(ps)
print()
print(ps.to_timestamp())
print()
```

示例了对象timestamp和period的转换过程，当字段不足会用默认值填充（见结果）

```txt
2018-01-31    0.259475
2018-02-28    0.128048
2018-03-31    0.553821
2018-04-30   -0.987806
2018-05-31    0.290521
Freq: M, dtype: float64

2018-01    0.259475
2018-02    0.128048
2018-03    0.553821
2018-04   -0.987806
2018-05    0.290521
Freq: M, dtype: float64

2018-01-01    0.259475
2018-02-01    0.128048
2018-03-01    0.553821
2018-04-01   -0.987806
2018-05-01    0.290521
Freq: MS, dtype: float64
```

## Series 多重索引

简而言之，如果你用一个多维数组去构建Series
Series中会自动匹配出多重索引
当然， 对输出操作上也可以切片

```python
letters = ['A', 'B', 'C']
numbers = list(range(10))
mi = pd.MultiIndex.from_product([letters, numbers])  # 设置多重索引
s = pd.Series(np.random.rand(30), index=mi)  # 随机数
print([letters, numbers])
print(s)
print()
print(s.loc[pd.IndexSlice[:'B', 5:]])
```

```
[['A', 'B', 'C'], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]
A  0    0.243756
   1    0.363313
   2    0.166970
   3    0.092553
   4    0.149003
   5    0.141381
   6    0.313245
   7    0.048826
   8    0.575368
   9    0.135024
B  0    0.687348
   1    0.853542
   2    0.396828
   3    0.084553
   4    0.656543
   5    0.291948
   6    0.310941
   7    0.779025
   8    0.644462
   9    0.764051
C  0    0.166460
   1    0.780991
   2    0.865861
   3    0.427068
   4    0.803411
   5    0.035250
   6    0.334675
   7    0.258239
   8    0.469680
   9    0.661562
dtype: float64

A  5    0.141381
   6    0.313245
   7    0.048826
   8    0.575368
   9    0.135024
B  5    0.291948
   6    0.310941
   7    0.779025
   8    0.644462
   9    0.764051
dtype: float64
```

## DataFrame 以多重索引创建

```python
frame = pd.DataFrame(np.arange(12).reshape(6, 2),
                     index=[list('AAABBB'), list('123123')],
                     columns=['hello', 'shiyanlou'])

print(frame)

```

通过np.arange(12).reshape(6, 2)创建12数字的向量修剪为6-2矩阵
指定aaabbb123123作为多重索引创建dataframe

```txt
     hello  shiyanlou
A 1      0          1
  2      2          3
  3      4          5
B 1      6          7
  2      8          9
  3     10         11
```
事实上索引也是可以指派名字的
可以通过frame.index.names = ['first', 'second']修改列名

## 分组求和，数据入栈
```python
frame = pd.DataFrame(np.arange(12).reshape(6, 2),
                     index=[list('AAABBB'), list('123123')],
                     columns=['hello', 'shiyanlou'])
frame.index.names = ['first', 'second']
print(frame)
print()
print(frame.groupby('first').sum())
print()
print(frame)
print()
print(frame.stack())
```
               
```txt
              hello  shiyanlou
first second                  
A     1           0          1
      2           2          3
      3           4          5
B     1           6          7
      2           8          9
      3          10         11

       hello  shiyanlou
first                  
A          6          9
B         24         27

              hello  shiyanlou
first second                  
A     1           0          1
      2           2          3
      3           4          5
B     1           6          7
      2           8          9
      3          10         11

first  second           
A      1       hello         0
               shiyanlou     1
       2       hello         2
               shiyanlou     3
       3       hello         4
               shiyanlou     5
B      1       hello         6
               shiyanlou     7
       2       hello         8
               shiyanlou     9
       3       hello        10
               shiyanlou    11
dtype: int32
```
在指派了索引名字后
frame.groupby('first')按照first索引分组求和
在这里就是输出结果按照AB各自区域求和
frame.stack()能够进行数据入栈，输出结果可以了解到数据被按照线性排列

## stack->unstack

所以通过出栈的方式还原也没问题

```python
frame = pd.DataFrame(np.arange(12).reshape(6, 2),
                     index=[list('AAABBB'), list('123123')],
                     columns=['hello', 'shiyanlou'])
frame.index.names = ['first', 'second']


f2=frame.stack()
print(f2)
print()
f3=f2.unstack()
print(f3)

```

```txt
first  second           
A      1       hello         0
               shiyanlou     1
       2       hello         2
               shiyanlou     3
       3       hello         4
               shiyanlou     5
B      1       hello         6
               shiyanlou     7
       2       hello         8
               shiyanlou     9
       3       hello        10
               shiyanlou    11
dtype: int32

              hello  shiyanlou
first second                  
A     1           0          1
      2           2          3
      3           4          5
B     1           6          7
      2           8          9
      3          10         11
```

## 条件查找
```python

data = {'animal': ['cat', 'cat', 'snake', 'dog', 'dog', 'cat', 'snake', 'cat', 'dog', 'dog'],
        'age': [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],
        'visits': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],
        'priority': ['yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no']}

labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']
df = pd.DataFrame(data, index=labels)
print(df)
print()
#查找 `age` 大于 `3` 的全部信息
print(df[df['age'] > 3])
```

```txt
  animal  age  visits priority
a    cat  2.5       1      yes
b    cat  3.0       3      yes
c  snake  0.5       2       no
d    dog  NaN       3      yes
e    dog  5.0       2       no
f    cat  2.0       3       no
g  snake  4.5       1       no
h    cat  NaN       1      yes
i    dog  7.0       2       no
j    dog  3.0       1       no

  animal  age  visits priority
e    dog  5.0       2       no
g  snake  4.5       1       no
i    dog  7.0       2       no
```

### df 索引切片

和Series相比仅仅改变了方法名字
后面几个查询方法仅仅提供标准样例，不赘述

df.iloc[2:4, 1:3]

### DataFrame 按关键字查询

df3[df3['animal'].isin(['cat', 'dog'])]

###  DataFrame 按标签及列名查询。

df.loc[df2.index[[3, 4, 8]], ['animal', 'age']]

## DataFrame 多条件排序

```python
data = {'animal': ['cat', 'cat', 'snake', 'dog', 'dog', 'cat', 'snake', 'cat', 'dog', 'dog'],
        'age': [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],
        'visits': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],
        'priority': ['yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no']}

labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']
df = pd.DataFrame(data, index=labels)
#ascending中false表示age按照降序,true表示visits按照升序
print(df.sort_values(by=['age', 'visits'], ascending=[False, True]))
```

```txt
  animal  age  visits priority
i    dog  7.0       2       no
e    dog  5.0       2       no
g  snake  4.5       1       no
j    dog  3.0       1       no
b    cat  3.0       3      yes
a    cat  2.5       1      yes
f    cat  2.0       3       no
c  snake  0.5       2       no
h    cat  NaN       1      yes
d    dog  NaN       3      yes
```

## DataFrame 多值替换

map+字典可以按照字典进行替换

df['priority'].map({'yes': True, 'no': False})

## DataFrame 分组求和

.groupby().sum()方法，使用同理

## DataFrame 拼接

直接用列表组装多个df对象即可

```python
temp_df1 = pd.DataFrame(np.random.randn(5, 4))  # 生成由随机数组成的 DataFrame 1
temp_df2 = pd.DataFrame(np.random.randn(5, 4))  # 生成由随机数组成的 DataFrame 2
temp_df3 = pd.DataFrame(np.random.randn(5, 4))  # 生成由随机数组成的 DataFrame 3

print(temp_df1)
print(temp_df2)
print(temp_df3)

pieces = [temp_df1, temp_df2, temp_df3]
print(pd.concat(pieces))

```

```txt
 ====
          0         1         2         3
0 -0.264635 -0.052950  0.688170 -1.914366
1 -1.288670 -0.992253 -0.228969 -0.693502
2 -1.211159  0.021853 -1.598827  0.469818
3  0.895677  0.865189 -1.102262 -1.386708
4  0.620885  1.722960  0.215481 -0.103105
          0         1         2         3
0 -0.203192 -0.635650 -0.652312 -0.959497
1  1.345722 -1.453662  1.225791 -0.408523
2 -0.418114  0.155324  1.466646  0.020927
3 -0.485332  1.754933 -1.250501  0.147999
4 -0.909008 -0.265547 -0.280067  1.734646
          0         1         2         3
0  1.140604 -1.128826 -0.601169 -0.962397
1 -0.941003 -0.324124  2.310820 -0.072020
2 -1.296052  1.425566  0.201062  1.306156
3  0.135159 -0.708848  0.684413  1.045724
4  2.373536  1.428124 -0.359993  1.402373
          0         1         2         3
0 -0.264635 -0.052950  0.688170 -1.914366
1 -1.288670 -0.992253 -0.228969 -0.693502
2 -1.211159  0.021853 -1.598827  0.469818
3  0.895677  0.865189 -1.102262 -1.386708
4  0.620885  1.722960  0.215481 -0.103105
0 -0.203192 -0.635650 -0.652312 -0.959497
1  1.345722 -1.453662  1.225791 -0.408523
2 -0.418114  0.155324  1.466646  0.020927
3 -0.485332  1.754933 -1.250501  0.147999
4 -0.909008 -0.265547 -0.280067  1.734646
0  1.140604 -1.128826 -0.601169 -0.962397
1 -0.941003 -0.324124  2.310820 -0.072020
2 -1.296052  1.425566  0.201062  1.306156
3  0.135159 -0.708848  0.684413  1.045724
4  2.373536  1.428124 -0.359993  1.402373


```

## 找出 DataFrame 表中和最小的列

```python 

df = pd.DataFrame(np.random.random(size=(5, 10)), columns=list('abcdefghij'))
print(df)
print()
print(df.sum().idxmin())  # idxmax(), idxmin() 为 Series 函数返回最大最小值的索引值

```

随机生成十列随机数
用idmin idmax 方法可以输出最大最小的列

```txt
          a         b         c  ...         h         i         j
0  0.965979  0.151651  0.353497  ...  0.964158  0.285298  0.079833
1  0.763793  0.362908  0.792489  ...  0.649987  0.827829  0.853108
2  0.129140  0.505827  0.491931  ...  0.742919  0.807635  0.661179
3  0.243242  0.087891  0.909605  ...  0.507234  0.745540  0.133692
4  0.383921  0.063741  0.999688  ...  0.430012  0.763586  0.968491

[5 rows x 10 columns]

b
```

##  每个元素减去每一行的平均值

统计常用

```python
df = pd.DataFrame(np.random.random(size=(5, 3)))
print(df)
print(df.sub(df.mean(axis=1), axis=0))
```

```txt
          0         1         2
0  0.896731  0.822522  0.576800
1  0.796234  0.147140  0.445422
2  0.748368  0.394187  0.321203
3  0.349887  0.023185  0.080568
4  0.188878  0.215437  0.980657
          0         1         2
0  0.131380  0.057171 -0.188551
1  0.333302 -0.315792 -0.017510
2  0.260448 -0.093733 -0.166716
3  0.198674 -0.128028 -0.070645
4 -0.272779 -0.246220  0.518999
```

## 透视表pivot_table

当分析庞大的数据时，为了更好的发掘数据特征之间的关系，且不破坏原数据，就可以利用透视表 `pivot_table` 进行操作。



```python
df = pd.DataFrame({'A': ['one', 'one', 'two', 'three'] * 3,
                   'B': ['A', 'B', 'C'] * 4,
                   'C': ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2,
                   'D': np.random.randn(12),
                   'E': np.random.randn(12)})

print(df)
#AB作为索引进行聚合
print(pd.pivot_table(df, index=['A', 'B']))
```

实际上很好理解，就是在普通表的基础上按照某些规则进行透视，类似数据库中的table 和 view 的区别

```txt
        A  B    C         D         E
0     one  A  foo  0.228258 -0.516026
1     one  B  foo -0.856476 -1.479398
2     two  C  foo  1.509264  2.666746
3   three  A  bar  0.103100 -2.097659
4     one  B  bar  0.883371  0.430193
5     one  C  bar  0.301807 -0.686110
6     two  A  foo -1.031254  0.605358
7   three  B  foo  0.794412  0.231234
8     one  C  foo  0.579062  0.835692
9     one  A  bar -0.355812  2.394057
10    two  B  bar -1.436048  1.705509
11  three  C  bar  0.495632 -1.070289
                D         E
A     B                    
one   A -0.063777  0.939016
      B  0.013448 -0.524602
      C  0.440435  0.074791
three A  0.103100 -2.097659
      B  0.794412  0.231234
      C  0.495632 -1.070289
two   A -1.031254  0.605358
      B -1.436048  1.705509
      C  1.509264  2.666746
```


## 定义绝对类型

在数据的形式上主要包括数量型和性质型，数量型表示着数据可数范围可变，而性质型表示范围已经确定不可改变，绝对型数据就是性质型数据的一种。

```python
df = pd.DataFrame({"id": [1, 2, 3, 4, 5, 6], "raw_grade": [
                  'a', 'b', 'b', 'a', 'a', 'e']})
print(df)
df["grade"] = df["raw_grade"].astype("category")
print(df)

print(df.sort_values(by="grade"))

```

实质上就是非数字型数据，用一个字符串填充内容表示定性而不是定值
同样也可以使用sort_values方法进行排序

```txt
   id raw_grade
0   1         a
1   2         b
2   3         b
3   4         a
4   5         a
5   6         e
   id raw_grade grade
0   1         a     a
1   2         b     b
2   3         b     b
3   4         a     a
4   5         a     a
5   6         e     e
   id raw_grade grade
0   1         a     a
3   4         a     a
4   5         a     a
1   2         b     b
2   3         b     b
5   6         e     e
```

## 数据清洗

如果数据不是想要的就要丢或者增补
下面用实例**题目**作为参考

### 缺失值拟合

在`FilghtNumber`中有数值缺失，其中数值为按 10 增长，补充相应的缺省值使得数据完整，并让数据为 `int` 类型。

```python
df = pd.DataFrame({'From_To': ['LoNDon_paris', 'MAdrid_miLAN', 'londON_StockhOlm',
                               'Budapest_PaRis', 'Brussels_londOn'],
                   'FlightNumber': [10045, np.nan, 10065, np.nan, 10085],
                   'RecentDelays': [[23, 47], [], [24, 43, 87], [13], [67, 32]],
                   'Airline': ['KLM(!)', '<Air France> (12)', '(British Airways. )',
                               '12. Air France', '"Swiss Air"']})
df['FlightNumber'] = df['FlightNumber'].interpolate().astype(int)
print(df)

```

通过df['FlightNumber'] = df['FlightNumber'].interpolate().astype(int)缺失值按照要求得到了补充

```txt

            From_To  FlightNumber  RecentDelays              Airline
0      LoNDon_paris         10045      [23, 47]               KLM(!)
1      MAdrid_miLAN         10055            []    <Air France> (12)
2  londON_StockhOlm         10065  [24, 43, 87]  (British Airways. )
3    Budapest_PaRis         10075          [13]       12. Air France
4   Brussels_londOn         10085      [67, 32]          "Swiss Air"
```

### 数据列拆分

```python
其中`From_to`应该为两独立的两列`From`和`To`，将`From_to`依照`_`拆分为独立两列建立为一个新表。

df = pd.DataFrame({'From_To': ['LoNDon_paris', 'MAdrid_miLAN', 'londON_StockhOlm',
                               'Budapest_PaRis', 'Brussels_londOn'],
                   'FlightNumber': [10045, np.nan, 10065, np.nan, 10085],
                   'RecentDelays': [[23, 47], [], [24, 43, 87], [13], [67, 32]],
                   'Airline': ['KLM(!)', '<Air France> (12)', '(British Airways. )',
                               '12. Air France', '"Swiss Air"']})
df['FlightNumber'] = df['FlightNumber'].interpolate().astype(int)
temp = df.From_To.str.split('_', expand=True)
temp.columns = ['From', 'To']
print(temp)
```

```txt
       From         To
0    LoNDon      paris
1    MAdrid      miLAN
2    londON  StockhOlm
3  Budapest      PaRis
4  Brussels     londOn
```

### 字符标准化

其中注意到地点的名字都不规范（如：`londON`应该为`London`）需要对数据进行标准化处理。

```python
df = pd.DataFrame({'From_To': ['LoNDon_paris', 'MAdrid_miLAN', 'londON_StockhOlm',
                               'Budapest_PaRis', 'Brussels_londOn'],
                   'FlightNumber': [10045, np.nan, 10065, np.nan, 10085],
                   'RecentDelays': [[23, 47], [], [24, 43, 87], [13], [67, 32]],
                   'Airline': ['KLM(!)', '<Air France> (12)', '(British Airways. )',
                               '12. Air France', '"Swiss Air"']})
df['FlightNumber'] = df['FlightNumber'].interpolate().astype(int)
temp = df.From_To.str.split('_', expand=True)
temp.columns = ['From', 'To']
temp['From'] = temp['From'].str.capitalize()
temp['To'] = temp['To'].str.capitalize()
print(temp)
```

可以从结果中对比前面观察到字符变得标准化

```txt
       From         To
0    London      Paris
1    Madrid      Milan
2    London  Stockholm
3  Budapest      Paris
4  Brussels     London
```


### 删除坏数据加入整理好的数据

将最开始的 `From_to` 列删除，加入整理好的 `From` 和 `to` 列。

```python
df = pd.DataFrame({'From_To': ['LoNDon_paris', 'MAdrid_miLAN', 'londON_StockhOlm',
                               'Budapest_PaRis', 'Brussels_londOn'],
                   'FlightNumber': [10045, np.nan, 10065, np.nan, 10085],
                   'RecentDelays': [[23, 47], [], [24, 43, 87], [13], [67, 32]],
                   'Airline': ['KLM(!)', '<Air France> (12)', '(British Airways. )',
                               '12. Air France', '"Swiss Air"']})
df['FlightNumber'] = df['FlightNumber'].interpolate().astype(int)
temp = df.From_To.str.split('_', expand=True)
temp.columns = ['From', 'To']
temp['From'] = temp['From'].str.capitalize()
temp['To'] = temp['To'].str.capitalize()
print(df)
print()
df = df.drop('From_To', axis=1)
df = df.join(temp)
print(df)

```

```txt
            From_To  FlightNumber  RecentDelays              Airline
0      LoNDon_paris         10045      [23, 47]               KLM(!)
1      MAdrid_miLAN         10055            []    <Air France> (12)
2  londON_StockhOlm         10065  [24, 43, 87]  (British Airways. )
3    Budapest_PaRis         10075          [13]       12. Air France
4   Brussels_londOn         10085      [67, 32]          "Swiss Air"

   FlightNumber  RecentDelays              Airline      From         To
0         10045      [23, 47]               KLM(!)    London      Paris
1         10055            []    <Air France> (12)    Madrid      Milan
2         10065  [24, 43, 87]  (British Airways. )    London  Stockholm
3         10075          [13]       12. Air France  Budapest      Paris
4         10085      [67, 32]          "Swiss Air"  Brussels     London
```

### 去除多余字符

如同 `airline` 列中许多数据有许多其他字符，会对后期的数据分析有较大影响，需要对这类数据进行修正。

```python
df = pd.DataFrame({'From_To': ['LoNDon_paris', 'MAdrid_miLAN', 'londON_StockhOlm',
                               'Budapest_PaRis', 'Brussels_londOn'],
                   'FlightNumber': [10045, np.nan, 10065, np.nan, 10085],
                   'RecentDelays': [[23, 47], [], [24, 43, 87], [13], [67, 32]],
                   'Airline': ['KLM(!)', '<Air France> (12)', '(British Airways. )',
                               '12. Air France', '"Swiss Air"']})
df['FlightNumber'] = df['FlightNumber'].interpolate().astype(int)
temp = df.From_To.str.split('_', expand=True)
temp.columns = ['From', 'To']
temp['From'] = temp['From'].str.capitalize()
temp['To'] = temp['To'].str.capitalize()

print()
df = df.drop('From_To', axis=1)
df = df.join(temp)

df['Airline'] = df['Airline'].str.extract(
    '([a-zA-Z\s]+)', expand=False).str.strip()
print(df)

```

```txt
   FlightNumber  RecentDelays          Airline      From         To
0         10045      [23, 47]              KLM    London      Paris
1         10055            []       Air France    Madrid      Milan
2         10065  [24, 43, 87]  British Airways    London  Stockholm
3         10075          [13]       Air France  Budapest      Paris
4         10085      [67, 32]        Swiss Air  Brussels     London
```

### 格式规范

在 `RecentDelays` 中记录的方式为列表类型，由于其长度不一，这会为后期数据分析造成很大麻烦。这里将 `RecentDelays` 的列表拆开，取出列表中的相同位置元素作为一列，若为空值即用 `NaN` 代替。

```python
df = pd.DataFrame({'From_To': ['LoNDon_paris', 'MAdrid_miLAN', 'londON_StockhOlm',
                               'Budapest_PaRis', 'Brussels_londOn'],
                   'FlightNumber': [10045, np.nan, 10065, np.nan, 10085],
                   'RecentDelays': [[23, 47], [], [24, 43, 87], [13], [67, 32]],
                   'Airline': ['KLM(!)', '<Air France> (12)', '(British Airways. )',
                               '12. Air France', '"Swiss Air"']})
df['FlightNumber'] = df['FlightNumber'].interpolate().astype(int)
temp = df.From_To.str.split('_', expand=True)
temp.columns = ['From', 'To']
temp['From'] = temp['From'].str.capitalize()
temp['To'] = temp['To'].str.capitalize()

print()
df = df.drop('From_To', axis=1)
df = df.join(temp)

df['Airline'] = df['Airline'].str.extract(
    '([a-zA-Z\s]+)', expand=False).str.strip()

delays = df['RecentDelays'].apply(pd.Series)

delays.columns = ['delay_{}'.format(n)
                  for n in range(1, len(delays.columns)+1)]

df = df.drop('RecentDelays', axis=1).join(delays)

print(df)
```

```txt
   FlightNumber          Airline      From  ... delay_1  delay_2  delay_3
0         10045              KLM    London  ...    23.0     47.0      NaN
1         10055       Air France    Madrid  ...     NaN      NaN      NaN
2         10065  British Airways    London  ...    24.0     43.0     87.0
3         10075       Air France  Budapest  ...    13.0      NaN      NaN
4         10085        Swiss Air  Brussels  ...    67.0     32.0      NaN

[5 rows x 7 columns]

```

## 数据预处理

###  信息区间划分

班级一部分同学的数学成绩表，如下图所示
```python
df=pd.DataFrame({'name':['Alice','Bob','Candy','Dany','Ella','Frank','Grace','Jenny'],'grades':[58,83,79,65,93,45,61,88]})
```

但我们更加关心的是该同学是否及格，将该数学成绩按照是否`>60`来进行划分。

```python
df = pd.DataFrame({'name': ['Alice', 'Bob', 'Candy', 'Dany', 'Ella',
                            'Frank', 'Grace', 'Jenny'], 'grades': [58, 83, 79, 65, 93, 45, 61, 88]})


def choice(x):
    if x > 60:
        return 1
    else:
        return 0


df.grades = pd.Series(map(lambda x: choice(x), df.grades))
print(df)

```

```txt
    name  grades
0  Alice       0
1    Bob       1
2  Candy       1
3   Dany       1
4   Ella       1
5  Frank       0
6  Grace       1
7  Jenny       1
```

###  数据去重

一个列为`A`的 DataFrame 数据，如下图所示
```python
df = pd.DataFrame({'A': [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 7]})
```

尝试将 A 列中连续重复的数据清除。

```python
df = pd.DataFrame({'A': [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 7]})
df.loc[df['A'].shift() != df['A']]
print(df)
```

```txt
    A
0   1
1   2
2   2
3   3
4   4
5   5
6   5
7   5
8   6
9   7
10  7
```

### 数据归一化

有时候，DataFrame 中不同列之间的数据差距太大，需要对其进行归一化处理。

其中，Max-Min 归一化是简单而常见的一种方式

```python
def normalization(df):
    numerator = df.sub(df.min())
    denominator = (df.max()).sub(df.min())
    Y = numerator.div(denominator)
    return Y


df = pd.DataFrame(np.random.random(size=(5, 3)))
print(df)
print()
print(normalization(df))

```
公式写在了函数里面。结果中可以看到后者数据被归一化

```txt
          0         1         2
0  0.446514  0.358759  0.765691
1  0.923732  0.742474  0.195854
2  0.637601  0.390288  0.212695
3  0.657625  0.536214  0.587192
4  0.617639  0.546014  0.380033

          0         1         2
0  0.000000  0.000000  1.000000
1  1.000000  1.000000  0.000000
2  0.400419  0.082166  0.029554
3  0.442378  0.462466  0.686755
4  0.358589  0.488005  0.323213
```
