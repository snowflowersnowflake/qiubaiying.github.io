---
layout:     post
title:      svm与非线性规划
subtitle:   #
date:       2020-05-21
author:     y00
header-img: img/1.jpg
catalog: true
tags:
    - 机器学习
---

# svm

简单点讲，SVM就是一种二类分类模型，他的基本模型是的定义在特征空间上的间隔最大（max margin）的线性分类器，SVM的学习策略就是间隔最大化。

通俗理解svm模型在特征点分类上的过程，把特征点的集群比作一盒乒乓球的话，那就是将桌上的一堆乒乓球拍起来，然后用一个kernel（核，高维空间下低一维度的超平面）将不同颜色的乒乓球在高维空间上分割开来。

这里球的维度取决于数据点的特征，拍起来的过程相当于挖掘数据特征，分割的过程则是一个**非线性规划问题**

# 非线性规划(nonlinear programming)

若规划问题的目标函数或约束条件中包含非线性函数，则称为非线性规划。

非线性规划的最优解（若存在）可能在其可行域的任一点达到，目前非线性规划还没有适合各种问题的一般解法，各种方法都有其特定的适用范围。

对于各类非线性规划的表示形式，可以参考[这个链接](https://zhuanlan.zhihu.com/p/27007254)

svm讨论的主要是**不等式的非线性约束问题**，优化的目标是寻找到最大的间隔。

优化的约束即是“间隔两边的特征是都在间隔两边的”（xw+b<-1,xw+b>1，解出来的x就是对应的待分类特征点群的边界）。

当然也可能涉及到软间隔（soft margin）问题，即允许不同标签特征数据产生一定的交集，也就是可以说“允许一部分点越出间隔面”，对于软间隔问题，就采用构造罚项的方式，引入松弛因子，对于不同离群程度的离群点给予不同程度的惩罚（这里可以参考[这个链接](https://www.jianshu.com/p/8a499171baa9)）

# Karush-Kuhn-Tucker (KKT)条件

Karush-Kuhn-Tucker (KKT)条件是非线性规划(nonlinear programming)最佳解的必要条件。

KKT条件将Lagrange乘数法(Lagrange multipliers)所处理涉及等式的约束优化问题推广至不等式。

这里的largrange乘数法是等式优化问题的典型解法，比如对于一个等式约束: 找到解空间下的x，在**约束g（x）= 0 条件**（和下文互补松弛性有关）下 求 f（x）最小值，这里就可以构造Lagrangian函数L(x,l)=f（x）+l*g(x),这里l是lagrange乘数，问题就转换成了 求L（x,l）的最小值。这就是一个无约束的优化问题了，可以利用诸如梯度下降法之类的优化方法解决。

而对于不等式的优化问题，kkt条件则是包括了方程、原始可行性、对偶可行性、互补松弛性这几种约束在内的一组必要条件。

约束不等式g（x）<=0就是原始可行性，但因为在梯度不断下降的优化过程中，有gradf=△f+△g*l，这里g的偏导是指向外部的（△g指向g>0的区域，即偏离我们优化出最小值所取得参数的方向），我们需要让乘数l>=0，从而产生了对偶可行性的条件。这里l和g是符号相反的，好比爬山过程中鞋子穿反了（梯度为指向外部），所以相对于鞋子朝向要反方向移动才能往正确的梯度上移动。

而无论最佳解产生在哪里，都会满足互补松弛性l*g（x）= 0，上述几组条件合称为kkt条件。

虽然没有讨论多约束的推广和不同的kernel trick，但到这里svm优化过程基本就明确了，同时可以参考[这个链接](
https://zhuanlan.zhihu.com/p/49331510)来理解svm的细节和优缺点。
