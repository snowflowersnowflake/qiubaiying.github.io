---
layout:     post
title:      概率图模型体系
subtitle:   #
date:       2019-10-30
author:     y00
header-img: img/yukiasuna.jpg
catalog: true
tags:
    - 统计学
    - 机器学习
    - hmm
---

# 前言

最近各种意义上，处在了一个相当忙碌的状态，更新的不是很频繁

因为笔者的数理基础不是很好，在学习到统计模型理论的时候，对hmm（隐马模型）和crf（条件随机场）这方面的理论，感觉十分模糊。

~~为了不只会掉包~~，看不大懂国外文献的我，在知乎上寻找了不少人的回答，惊喜的是，确实有解释的相当生动的。

这里对相关链接资料以及概念做简单梳理（因为网上大量的灌水实在太多了），减少学习中的弯路

# 一张清晰的图

<img src="https://pic3.zhimg.com/v2-714c1843f78b6aecdb0c57cdd08e1c6a_r.jpg"/>

这张图清晰列举了，**统计概率图的体系结构**

* 有向图和无向图是基本的数据结构，自不必说区别
* 统计学分为频率派和贝叶斯派，贝叶斯派倾向针对事件的**不确定性**建模。[这个链接](https://www.zhihu.com/question/20587681/answer/41436978)描述的很详细
* 有向图分为静态贝叶斯网络和动态网络，**静态模型是指图随着时间不改变状态，动态是指随着时间推移改变状态**
* 在笔者看来，贝叶斯网络中图结构实际上是一个又一个用以描述事件中包含的**状态**的概率关系
* 无向图概率模型是将一组联合概率分布分解为局部概率分布，**除了相邻节点，其他节点条件独立**

# hmm

也称观测模型，可以通过一组先验观测结果，得到对下一组的估计

很多对于hmm的资料中，描述相对生涩，甚至是github上模型的中文文档，也不是非常清晰。

主要的困惑集中于：什么是隐含状态 && 模型解决的问题（输出内容） && 得到系统隐含状态的方法

# 困惑有所解除的时候

[这个回答用赌场投掷骰子的背景生动展现了hmm的特点](https://www.zhihu.com/question/20962240/answer/33561657)

写的真的非常棒，尽管这个背景未必是最适合用hmm建模的（评论指出），但以此确实思路很清晰

## 隐含状态是什么

系统的隐含状态是指外界观测不到的状态

例如一组台风移动的参数路径，来估计下一时段台风的去向

例如背景中，通过骰子投掷结果进行概率分析，确定哪一个骰子没有做手脚

## 模型解决的问题

hmm可以通过对系统隐性状态概率分布的学习，估计表现为某种状态的概率。

换句话说，如果有两种骰子（问题骰子产生的结果会不同于正常骰子的类似均等概率）。hmm可以基于对已有数据的学习，估计一组结果，更像哪种骰子。

听起来和大多数机器学习算法如出一辙不是吗。所以问题的关键是学习和估计的方法（对于学习既有数据表现的概率建议参考回答的推导）。这个模型也常用于nlp领域。

hmm也涉及到两个重要的概念，**隐性状态的转移概率**transmat_

例如我们指定一个选盒子的规则：

* 如果当前是盒子1，则选择盒子2
* 如果当前是盒子2或3，则分布以概率0.4和0.6选择前一个或后一个盒子
* 如果当前是盒子4，则各以0.5的概率停留在盒子4或者选择盒子3

那么这个问题下的transmat就是

#|q1|q2|q3|q4
---|---|---|---|---
q1|0|1|0|0
q2|0.4|0|0.6|0
q3|0|0.4|0|0.6
q4|0|0|0.5|0.5

## 估计方法

实际上并不复杂，用上面文章的回答来说，就是经过假设，然后进行条件概率的累乘

比如在建立了模型，可以描述两个骰子对不同结果的概率分布的情况下

给定可见序列v(1~i)，我们想知道这位大叔下一局vi+1连续掷出10个6的概率是多少?

那么我们对所有情况可以一一假设，（前面abcd...个是正常骰子，efgh...个是出千的骰子）

我们对这个未知的序列就打了如上的01标签，这个序列就称作**隐性状态序列w**，例如假设大叔前5个用的是正常骰子, 后5个用的是作弊骰子1.

也就是说，如果假设所有情况，我们就有了x个隐性序列x*w。

对于这些序列，可以一一根据条件概率的组合计算出序列出现的总概率。

btw，具体实现的过程中，并不会假设全部的序列，会根据算法中的规则简化复杂度。

再利用[维特比算法](https://zh.wikipedia.org/wiki/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95)，基于v和所有w的关系，选出最可能的w

最终抉择出来的w就是我们的预测结果了

# crf条件随机场

crf模型和hmm最直接的区别就是，提供了一整套方案来实现对观测序列特征的学习，实际使用的时候可以把feature放在序列的节点或者边上面。

限于精力不够，就放出这个补充连接了

[条件随机场（CRF）和隐马尔科夫模型（HMM）最大区别在哪里？CRF的全局最优体现在哪里？ - 熊辰炎的回答 - 知乎](https://www.zhihu.com/question/53458773/answer/330396666)

# hmmlearn

一个推断hmm的开源模型

* 库中包括了GaussianHMM、GMMHMM、MultinomialHMM三种，根据观测类型的不同分布(distribution)可以选用不同的模型进行序列预测

* 模型地址在[这个链接](https://github.com/hmmlearn/hmmlearn/tree/master/examples)

可以按照如下方式build（核心部分），参数细节可以参考[帮助文档](http://www.huaxiaozhuan.com/%E5%B7%A5%E5%85%B7/scikit-learn/chapters/7.HMM.html)：

```python
model = hmm.GaussianHMM(n_components=4, covariance_type="full")

# Instead of fitting it from the data, we directly set the estimated
# parameters, the means and covariance of the components
model.startprob_ = startprob
model.transmat_ = transmat
model.means_ = means
model.covars_ = covars
```

* n_components代表了状态的数量
* startprob_用一个数组指定初始状态的先验概率分布

startprob = np.array([0.6, 0.3, 0.1, 0.0])

这个栗子就对应了4个components的probability

* transmat_用矩阵表示先验的状态转移概率 (n_components, n_components)
```python
transmat = np.array([[0.7, 0.2, 0.0, 0.1],
                     [0.3, 0.5, 0.2, 0.0],
                     [0.0, 0.3, 0.5, 0.2],
                     [0.2, 0.0, 0.2, 0.6]])
```
这个矩阵中，描述了4种状态相互转移的4*4种可能性

* means是每个状态的均值参数(n_components,n_features特征维度 )
* covars_是每个状态的方差参数，形状不定

**这两个对象实际上就和我们的实际观测序列有关**

```python
means = np.array([[0.0,  0.0],
                  [0.0, 11.0],
                  [9.0, 10.0],
                  [11.0, -1.0]])
                  
covars = .5 * np.tile(np.identity(2), (4, 1, 1))                  
```

在这个初始化中，means针对于4种状态（每个状态有两个特征）指定均值，用np的方法通过单位矩阵生成状态的方差

